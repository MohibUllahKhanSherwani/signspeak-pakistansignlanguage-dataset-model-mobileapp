# train_model_with_augmentation.py
"""
Enhanced training script with data augmentation support.

Features:
- Optional data augmentation (3x multiplier by default)
- Progress tracking
- Model checkpointing
- Training history visualization
"""

import numpy as np
import os
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import joblib
import argparse

from actions_config import (
    load_actions, DATA_PATH, SEQUENCE_LENGTH, 
    NUM_SEQUENCES, BATCH_SIZE, EPOCHS, LEARNING_RATE, AUGMENTATION_MULTIPLIER
)
from data_augmentation import create_augmented_dataset


def load_data(actions):
    """Load collected sequence data."""
    sequences, labels = [], []
    
    print("\nüìÇ Loading data...")
    for action in actions:
        action_path = os.path.join(DATA_PATH, action)
        if not os.path.exists(action_path):
            raise FileNotFoundError(f"Data folder missing for action: {action}")
        
        # Count actual sequences
        actual_seqs = len([d for d in os.listdir(action_path) if d.isdigit()])
        print(f"  ‚Ä¢ {action}: {actual_seqs} sequences")
        
        for seq in range(actual_seqs):
            window = []
            for frame_num in range(SEQUENCE_LENGTH):
                npy_path = os.path.join(action_path, str(seq), f"{frame_num}.npy")
                if not os.path.exists(npy_path):
                    raise FileNotFoundError(f"Missing file: {npy_path}")
                frame = np.load(npy_path)
                window.append(frame)
            sequences.append(window)
            labels.append(action)
    
    return np.array(sequences), np.array(labels)


def build_model(input_shape, num_classes, use_dropout=True):
    """
    Build LSTM model with optional dropout for regularization.
    
    Args:
        input_shape: (sequence_length, num_features)
        num_classes: Number of sign classes
        use_dropout: Add dropout layers (recommended for augmented data)
    """
    model = Sequential()
    
    # LSTM layers
    model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=input_shape))
    if use_dropout:
        model.add(Dropout(0.2))
    
    model.add(LSTM(128, return_sequences=True, activation='relu'))
    if use_dropout:
        model.add(Dropout(0.2))
    
    model.add(LSTM(64, return_sequences=False, activation='relu'))
    if use_dropout:
        model.add(Dropout(0.2))
    
    # Dense layers
    model.add(Dense(64, activation='relu'))
    if use_dropout:
        model.add(Dropout(0.3))
    
    model.add(Dense(32, activation='relu'))
    
    # Output layer
    model.add(Dense(num_classes, activation='softmax'))
    
    # Compile
    optimizer = Adam(learning_rate=LEARNING_RATE)
    model.compile(
        optimizer=optimizer,
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model


def main():
    parser = argparse.ArgumentParser(description='Train PSL recognition model')
    parser.add_argument(
        '--augment',
        action='store_true',
        help='Use data augmentation (recommended for small datasets)'
    )
    parser.add_argument(
        '--augment-multiplier',
        type=int,
        default=AUGMENTATION_MULTIPLIER,
        help=f'Augmentation multiplier (default: {AUGMENTATION_MULTIPLIER}x)'
    )
    parser.add_argument(
        '--no-dropout',
        action='store_true',
        help='Disable dropout layers'
    )
    parser.add_argument(
        '--epochs',
        type=int,
        default=EPOCHS,
        help=f'Number of epochs (default: {EPOCHS})'
    )
    
    args = parser.parse_args()
    
    # Load actions
    actions = load_actions()
    print(f"\nüéØ Training model for {len(actions)} actions:")
    print(f"   {', '.join(actions)}")
    
    # Load data
    X, y = load_data(actions)
    print(f"\nüìä Original dataset shape: {X.shape}")
    print(f"   ‚Ä¢ {len(X)} sequences")
    print(f"   ‚Ä¢ {X.shape[1]} frames per sequence")
    print(f"   ‚Ä¢ {X.shape[2]} features per frame")
    
    # Apply augmentation if requested
    if args.augment:
        print(f"\nüîÑ Applying data augmentation (x{args.augment_multiplier})...")
        X_augmented, y_augmented = create_augmented_dataset(
            X, y, 
            augmentation_multiplier=args.augment_multiplier
        )
        print(f"   ‚úÖ Augmented dataset: {X_augmented.shape}")
        print(f"   ‚úÖ Increased from {len(X)} to {len(X_augmented)} sequences!")
        X, y = X_augmented, y_augmented
    else:
        print("\n‚ö†Ô∏è  Training WITHOUT augmentation")
        print("   Tip: Use --augment for better performance with small datasets")
    
    # Encode labels
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    y_cat = to_categorical(y_encoded).astype(int)
    
    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_cat,
        test_size=0.2,
        random_state=42,
        stratify=y_encoded
    )
    
    print(f"\nüìà Train/Test Split:")
    print(f"   ‚Ä¢ Training: {len(X_train)} sequences")
    print(f"   ‚Ä¢ Testing: {len(X_test)} sequences")
    
    # Build model
    input_shape = (X_train.shape[1], X_train.shape[2])
    use_dropout = not args.no_dropout
    
    print(f"\nüèóÔ∏è  Building model...")
    print(f"   ‚Ä¢ Input shape: {input_shape}")
    print(f"   ‚Ä¢ Num classes: {y_cat.shape[1]}")
    print(f"   ‚Ä¢ Dropout: {'Enabled' if use_dropout else 'Disabled'}")
    
    model = build_model(input_shape, y_cat.shape[1], use_dropout=use_dropout)
    model.summary()
    
    # Callbacks
    callbacks = []
    
    # Model checkpoint (save best model)
    checkpoint = ModelCheckpoint(
        'best_action_model.h5',
        monitor='val_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    )
    callbacks.append(checkpoint)
    
    # Early stopping (stop if no improvement)
    early_stop = EarlyStopping(
        monitor='val_accuracy',
        patience=30,
        mode='max',
        verbose=1,
        restore_best_weights=True
    )
    callbacks.append(early_stop)
    
    # Reduce learning rate on plateau
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=10,
        min_lr=1e-6,
        verbose=1
    )
    callbacks.append(reduce_lr)
    
    # Train
    print(f"\nüöÄ Starting training for {args.epochs} epochs...")
    print("=" * 60)
    
    history = model.fit(
        X_train, y_train,
        epochs=args.epochs,
        batch_size=BATCH_SIZE,
        validation_data=(X_test, y_test),
        callbacks=callbacks,
        verbose=1
    )
    
    # Save final model
    model.save("action_model.h5")
    joblib.dump(le, "label_encoder.pkl")
    
    # Print results
    print("\n" + "=" * 60)
    print("‚úÖ Training complete!")
    print("=" * 60)
    
    final_train_acc = history.history['accuracy'][-1]
    final_val_acc = history.history['val_accuracy'][-1]
    
    print(f"\nüìä Final Results:")
    print(f"   ‚Ä¢ Training Accuracy: {final_train_acc * 100:.2f}%")
    print(f"   ‚Ä¢ Validation Accuracy: {final_val_acc * 100:.2f}%")
    
    print(f"\nüíæ Saved files:")
    print(f"   ‚Ä¢ action_model.h5 (final model)")
    print(f"   ‚Ä¢ best_action_model.h5 (best model during training)")
    print(f"   ‚Ä¢ label_encoder.pkl")
    
    # Tips
    print(f"\nüí° Tips:")
    if not args.augment:
        print("   ‚Ä¢ Try training with --augment for better generalization")
    if final_val_acc < 0.85:
        print("   ‚Ä¢ Low accuracy? Try collecting more data or increasing augmentation")
    if abs(final_train_acc - final_val_acc) > 0.15:
        print("   ‚Ä¢ Large train/val gap? Model may be overfitting")
        print("   ‚Ä¢ Consider using dropout or more augmentation")
    
    print("\nüéâ Ready for inference!")


if __name__ == "__main__":
    main()
